:_module-type: PROCEDURE

[id="enabling-the-observability-stack_{context}"]
= Enabling the observability stack

[role="_abstract"]
The observability stack collects and correlates metrics, traces, and alerts for {productname-short} so that you can monitor, troubleshoot, and optimize the platform. This capability is disabled by default on self-managed deployments. A cluster administrator enables it by updating the `DataScienceClusterInitialization` (DSCI) custom resource.

Once enabled, you can perform the following actions:

* Accelerate troubleshooting by viewing metrics, traces, and alerts for {productname-short} components in one place.
* Maintain platform stability by monitoring health and resource usage and receiving built-in alerts for critical issues.
* Integrate with existing tools by exporting telemetry to third-party observability solutions through OpenTelemetry.

.Prerequisites
* You have cluster administrator privileges for your {openshift-platform} cluster.
* You have installed {productname-long}.
* You have installed the following Operators, which provide the components of the observability stack:
** *Cluster Observability Operator*: Deploys and manages Prometheus and Alertmanager for metrics and alerts.
** *Tempo Operator*: Provides the Tempo backend for distributed tracing.
** *Red Hat build of OpenTelemetry*: Deploys the OpenTelemetry Collector for collecting and exporting telemetry data.

.Procedure
. Log in to the {openshift-platform} web console as a cluster administrator.
. In the {openshift-platform} console, click *Operators* → *Installed Operators*.
. Search for the *{productname-long}* Operator, and then click the Operator name to open the Operator details page.
. Click the *DSCInitialization* tab.
. Click the default instance name (for example, *default-dsci*) to open the instance details page.
. Click the *YAML* tab to show the instance specifications.
. In the `spec.components`, set the value of the `managementState` field for `monitoring` to `Managed`, and configure metrics, alerting, and tracing components.
+
.Example monitoring configuration
[source,yaml]
----
# ...
spec:
  monitoring:
    managementState: Managed                 # Required: Enables and manages the observability stack
    namespace: {monitoring-default-namespace}  # Required: Namespace where monitoring components are deployed
    alerting: {}                              # Alertmanager configuration. Uses default settings if left empty.
    metrics:                                  # Prometheus configuration for metrics collection
      replicas: 1                             # Optional: Number of Prometheus instances
      resources:                              # CPU and memory requests and limits for Prometheus pods
        cpulimit: 500m                        # Optional: Maximum CPU allocation in millicores
        cpurequest: 100m                      # Optional: Minimum CPU allocation in millicores
        memorylimit: 512Mi                    # Optional: Maximum memory allocation in mebibytes
        memoryrequest: 256Mi                  # Optional: Minimum memory allocation in mebibytes
      storage:                                # Storage configuration for metrics data
        size: 5Gi                             # Required: Storage size for Prometheus data
        retention: 90d                        # Required: Retention period for metrics data in days
      exporters: {}                           # External metrics exporters
    traces:                                   # Tempo backend for distributed tracing
      sampleRatio: '0.1'                      # Optional: Portion of traces to sample, expressed as a decimal
      storage:                                # Storage configuration for trace data
        backend: pv                           # Required: Storage backend for Tempo traces (pv, s3, or gcs)
        retention: 2160h                      # Optional: Retention period for trace data in hours
      exporters: {}                           # External trace exporters
# ...
----
. Click *Save* to apply your changes.

.Verification

Verify that the monitoring workloads are running in the configured namespace:
. In the {openshift-platform} web console, click *Workloads* → *Pods*.
. From the project list, select *{monitoring-default-namespace}*.
. Confirm that there are running pods for your configuration. The following pods indicate that the observability stack is active:
+
[source,terminal]
----
alertmanager-data-science-monitoringstack-#      2/2   Running   0   1m
data-science-collector-collector-#               1/1   Running   0   1m
prometheus-data-science-monitoringstack-#        2/2   Running   0   1m
tempo-data-science-tempomonolithic-#             1/1   Running   0   1m
thanos-querier-data-science-thanos-querier-#     2/2   Running   0   1m
----

. Optional: Verify that metrics are collected from instrumented components by confirming that their pods include the label `monitoring.opendatahub.io/scrape='true'`.


metrics
CRD > search for collector > OpenTelemetryCollector > looks for label on pod: monitoring.opendatahub.io/scrape=true 
  (example trustyai pod YAML, labels: monitoring.opendatahub.io/scrape: 'true'


* To collect metrics from your own workloads, you must label the pods that expose metrics endpoints:
+
[source,terminal]
----
oc label pod <pod_name> monitoring.opendatahub.io/scrape='true'
----
+
Pods with this label are automatically scraped by the Prometheus instance in the centralized monitoring stack.

[NOTE]
====
The `monitoring.opendatahub.io/scrape='true'` label is not added automatically when you enable the observability stack.  
This label is a temporary mechanism used by the OpenTelemetry Collector to identify which pods to scrape.  
Future releases are expected to replace this with automatic discovery based on ServiceMonitor and PodMonitor resources.
====