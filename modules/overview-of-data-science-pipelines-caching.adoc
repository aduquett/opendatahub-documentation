:_module-type: CONCEPT

[id='overview-of-data-science-pipelines-caching_{context}']
= Overview of data science pipelines caching

[role='_abstract']
You can use caching within data science pipelines to optimize execution times and improve resource efficiency. Caching reduces redundant task execution by reusing results from previous runs with identical inputs. 

Caching is particularly beneficial for iterative tasks, where intermediate steps might not need to be repeated. Understanding caching can help you design more efficient pipelines and save time in model development.

Caching operates by storing the outputs of successfully completed tasks and comparing the inputs of new tasks against previously cached ones. If a match is found, {productname-short} reuses the cached results instead of re-executing the task, reducing computation time and resource usage.

== Caching criteria

For caching to be effective, the following criteria determine if a task can use previously cached results:

* *Input data and parameters*: If the input data and parameters for a task are unchanged from a previous run, cached results are eligible for reuse.
* *Task code and configuration*: Changes to the task code or configurations invalidate the cache to ensure that modifications are always reflected.
* *Pipeline environment*: Changes to the pipeline environment, such as dependency versions, also affect caching eligibility to maintain consistency.

== Viewing cached steps in the {productname-short} user interface

Cached steps in pipelines are visually indicated in the user interface (UI):

* Tasks that use cached results display a green icon, helping you quickly identify which steps were cached. The *Status* field in the side panel displays `Cached` for cached tasks.  
* The UI also includes information about when the task was previously executed, allowing for easy verification of cache usage.

To confirm caching status for specific tasks, navigate to the pipeline details view in the UI, where all cached and non-cached tasks are indicated. When a pipeline task is cached, its execution logs are not available. This is because the task uses previously generated outputs, eliminating the need for re-execution.

== Disabling caching

ifdef::upstream,self-managed[]
In {productname-short}, caching is enabled by default to improve performance, but there are situations where you might want to disable caching for specific tasks, an entire pipeline, or all pipelines. For example, caching might not be beneficial for tasks that rely on frequently updated data or unique computational needs. In other cases, such as debugging, development, or when deterministic re-execution is required, you might want to disable caching for all pipelines.

[CAUTION]
====
Disabling caching at the pipeline or pipeline server level causes all tasks to run again, potentially increasing compute time and resource usage.
====
endif::[]

ifdef::cloud-service[]
In {productname-short}, caching is enabled by default to improve performance, but there are situations where you might want to disable caching for specific tasks or the entire pipeline. For example, caching might not be beneficial for tasks that rely on frequently updated data or unique computational needs. 

[CAUTION]
====
Disabling caching at the pipeline level causes all tasks to run again, potentially increasing compute time and resource usage.
====
endif::[]

You can control caching for data science pipelines in the following ways:

* *Individual task*: Data scientists can disable caching for specific steps in a pipeline.
* *Pipeline (submit time)*: Data scientists can disable caching when submitting a pipeline run.
* *Pipeline (compile time)*: Data scientists can disable caching when compiling a pipeline.
ifdef::upstream,self-managed[]
* *All pipelines (pipeline server)*: Cluster administrators can disable caching for all pipelines in the pipeline server.
endif::[]

=== Disabling caching for individual tasks

To disable caching for a particular task, apply the `set_caching_options` method directly to the task in your pipeline code:

`task_name.set_caching_options(False)`

After applying this setting, {productname-short} runs the task in future pipeline runs, ignoring any cached results.

[NOTE]
====
You can re-enable caching for individual tasks by setting `set_caching_options(True)`.
====

ifdef::upstream,self-managed[]
[NOTE]
====
This setting is ignored if caching is disabled in the pipeline server.
====
endif::[]

=== Disabling caching for a pipeline at submit time

To disable caching for the entire pipeline during pipeline submission, set the `enable_caching` parameter to `False` in your pipeline code. This setting ensures that no steps are cached during pipeline execution. The `enable_caching` parameter is available only when using the `kfp.client` to submit pipelines or start pipeline runs, such as the `run_pipeline` method.

Example:

`pipeline_func(enable_caching=False)`

ifdef::upstream,self-managed[]
[NOTE]
====
This setting is ignored if caching is disabled during pipeline compilation or in the pipeline server.
====
endif::[]

ifdef::cloud-service[]
[NOTE]
====
This setting is ignored if caching is disabled during pipeline compilation.
====
endif::[]

=== Disabling caching for a pipeline at compile time

To disable caching for the entire pipeline during compilation, set one of the following options in your local environment or workbench:

* Environment variable:
+
[source,bash]
----
export KFP_DISABLE_EXECUTION_CACHING_BY_DEFAULT=true
----

* CLI flag (when using `kfp dsl compile`):
+
[source,bash]
----
kfp dsl compile --disable-execution-caching-by-default
----

ifdef::upstream,self-managed[]
[NOTE]
====
This setting is ignored if caching is disabled in the pipeline server.
====
endif::[]

ifdef::upstream,self-managed[]
=== Disabling caching for all pipelines (pipeline server)

Cluster administrators can disable caching for all pipelines in the pipeline server, which overrides all pipeline and task-level caching settings.

In the OpenShift console or CLI, set the `cacheEnabled` field to `false` in the `DataSciencePipelinesApplication` (DSPA) custom resource for the project. 

Example:

[source,yaml]
----
apiVersion: datasciencepipelinesapplications.opendatahub.io/v1
kind: DataSciencePipelinesApplication
metadata:
  name: my-dspa
  namespace: my-namespace
spec:
  apiServer:
    cacheEnabled: false
----

[NOTE]
====
Changing this setting updates the `CACHEENABLED` environment variable in the pipeline server deployment. 
====
endif::[]

== Verification and troubleshooting

After configuring caching settings, you can verify that caching behaves as expected by using one of the following methods:

* *Checking the UI*: Confirm cached steps by locating the steps with the green icon in the task list.
* *Testing task re-runs*: Disable caching on individual tasks or the pipeline and check for re-execution to verify cache bypassing.
* *Validating inputs*: Ensure the task inputs, parameters, and environment remain unchanged if caching applies.

[role="_additional-resources"]
.Additional resources
* Kubeflow caching documentation: link:https://www.kubeflow.org/docs/components/pipelines/user-guides/core-functions/caching/[Kubeflow Pipelines - Use Caching]
* The `kfp.client` module documentation for the `enable_caching` parameter: link:https://kubeflow-pipelines.readthedocs.io/en/stable/source/client.html#kfp.client.Client.run_pipeline.enable_caching[KFP SDK API Reference - kfp.client]

