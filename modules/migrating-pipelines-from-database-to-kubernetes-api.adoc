:_module-type: PROCEDURE

[id="migrating-pipelines-from-database-to-kubernetes-api_{context}"]
= Migrating pipelines from database to Kubernetes API storage

[role="_abstract"]
You can migrate existing pipelines and pipeline versions from the internal database to Kubernetes custom resources. This makes it easier to use OpenShift GitOps (Argo CD) or similar tools to manage pipelines and pipeline versions, while still allowing you to manage them through the {productname-short} dashboard, API, and the Kubeflow Pipelines (KFP) Software Development Kit (SDK).

This procedure uses a community-supported Kubeflow Pipelines migration script to export pipelines from the Data Science Pipelines API and generate corresponding `Pipeline` and `PipelineVersion` custom resources for import into your cluster.

ifndef::upstream[]
[IMPORTANT]
====
The migration script in this procedure is maintained by the Kubeflow Pipelines community and is not supported by {org-name}. Before you use the script, review the repository and validate it in a non-production environment.
====
endif::[]

[WARNING]
====
The pipeline and pipeline version IDs change during migration, so existing pipeline runs do not map to the migrated pipeline version. The original ID is stored in the `pipelines.kubeflow.org/original-id` label.
====

.Prerequisites
* You have {productname-short} administrator privileges or you are the project owner.
* You have a data science project with a running pipeline server.
* The pipeline server is configured with `spec.apiServer.pipelineStore: database`.
* You have Python 3.11 installed in your local environment.
* You have installed the {openshift-cli} as described in the appropriate documentation for your cluster:
ifdef::upstream,self-managed[]
** link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Container Platform  
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-productname}
endif::[]
ifdef::cloud-service[]
** link:https://docs.redhat.com/en/documentation/openshift_dedicated/{osd-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Dedicated  
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws_classic_architecture/{rosa-classic-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-classic-productname}
endif::[]

.Procedure

. In a terminal window, log in to your {openshift-platform} cluster by using the {openshift-cli}:
+
[source,subs="+quotes"]
----
$ oc login -u __<user_name>__
----
+
When prompted, enter the {openshift-platform} server URL, connection type, and your password.

. Set environment variables for your data science project and get the pipeline API route.
+
In the `export` command, replace __<namespace>__ with the name of your project:
+
[source,subs="+quotes"]
----
echo "Setting the prerequisite variables"
export NAMESPACE=__<namespace>__
export DSPA_NAME=$(oc -n $NAMESPACE get dspa -o jsonpath='{.items[0].metadata.name}')
export API_URL="https://$(oc -n $NAMESPACE get route "ds-pipeline-$DSPA_NAME" -o jsonpath='{.spec.host}')"
----

. Create a Python virtual environment and install the required dependencies.
+
[source,subs="+quotes"]
----
echo "Set up the Python prerequisites"
python3.11 -m venv .venv
./.venv/bin/pip install kfp requests PyYAML
----

. Download and run the Kubeflow Pipelines community migration script.
+
The script connects to the Data Science Pipelines API, exports all pipelines and versions from the specified data science project, and generates one YAML file per pipeline in a local `kfp-exported-pipelines/` directory. Each file includes a `Pipeline` resource followed by all associated `PipelineVersion` resources.

.. Run the following command:
+
[source,subs="+quotes"]
----
curl -L https://raw.githubusercontent.com/kubeflow/pipelines/refs/heads/master/tools/k8s-native/migration.py -o migration.py
./.venv/bin/python migration.py --skip-tls-verify --kfp-server-host $API_URL --namespace $NAMESPACE --token "$(oc whoami --show-token)"
----
+
[NOTE]
====
The `--skip-tls-verify` option disables certificate validation and should be used only in development environments or when connecting to a server with a self-signed certificate. In production environments, provide a valid certificate bundle instead.

Additionally, passing the access token directly on the command line might expose it in shell history or process lists. To reduce this risk, store the token in an environment variable and reference it in your command:

[source,subs="+quotes"]
----
export KFP_TOKEN=$(oc whoami --show-token)
./.venv/bin/python migration.py --kfp-server-host $API_URL --namespace $NAMESPACE --token "$KFP_TOKEN"
----

Alternatively, use a prompt with `read -s` to input the token securely at runtime.
====

.. Optional: For more information about the script, run the following command:
+
[source]
----
./.venv/bin/python migration.py --help
----

.. If you plan to create new or updated `PipelineVersion` custom resources after migration, you can compile your pipeline code by using the Kubeflow Pipelines SDK. For more information, see _Compiling the pipeline YAML with the Kubeflow Pipelines SDK_ and _Compiling Kubernetes-native manifests with the Kubeflow Pipelines SDK_.

. Apply the exported Kubernetes custom resources to your cluster.
+
[source,subs="+quotes"]
----
oc apply -f ./kfp-exported-pipelines
----

. Change the pipeline server to use Kubernetes API storage.
+
[source,subs="+quotes"]
----
oc -n "$NAMESPACE" patch dspa "$DSPA_NAME" --type=merge -p '{"spec":{"apiServer":{"pipelineStore":"kubernetes"}}}'
----
+
[NOTE]
====
To view pipelines that were stored in the internal database and not migrated, you can temporarily change the pipeline server back to `database` storage.

[source,subs="+quotes"]
----
oc -n $NAMESPACE patch dspa $DSPA_NAME --type=merge -p '{"spec":{"apiServer":{"pipelineStore":"database"}}}'
----
====

. Repeat this procedure for each additional data science project that you want to migrate, changing `NAMESPACE` to the appropriate project name.

. Optional: Clean up the local environment.
+
[source,subs="+quotes"]
----
rm -rf .venv migration.py
----

.Verification
. Check that the `Pipeline` and `PipelineVersion` custom resources were created in your project:
+
[source,subs="+quotes"]
----
$ oc -n __<namespace>__ get pipelines.pipelines.kubeflow.org
$ oc -n __<namespace>__ get pipelineversions.pipelines.kubeflow.org
----

. Verify that the pipeline server is using Kubernetes API storage:
+
[source,subs="+quotes"]
----
$ oc -n __<namespace>__ get dspa __<dspa_name>__ -o jsonpath='{.spec.apiServer.pipelineStore}{"\n"}'
----
+
The command should return `kubernetes`.

.Additional resources
* link:https://github.com/kubeflow/pipelines/blob/master/tools/k8s-native/migration.py[Kubeflow Pipelines community migration script source^]
* link:https://raw.githubusercontent.com/kubeflow/pipelines/refs/heads/master/tools/k8s-native/README.md[Script README and usage details^]