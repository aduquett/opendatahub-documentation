:_module-type: PROCEDURE

[id='deploying-a-model-from-the-model-catalog_{context}']
= Deploying a model from the model catalog

[role='_abstract']
You can deploy models directly from the model catalog. 

[NOTE]
====
{productname-short} model serving deployments use the global cluster pull secret to pull models in ModelCar format from the catalog. 

ifdef::upstream,self-managed[]
For more information about using pull secrets in {openshift-platform}, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/images/managing-images#images-update-global-pull-secret_using-image-pull-secrets[Updating the global cluster pull secret] in the {openshift-platform} documentation.
endif::[]
====

.Prerequisites
ifdef::upstream[]
* You have completed the prerequisites in link:{odhdocshome}/deploying-models/#deploying-models-on-the-single-model-serving-platform_odh-user[Deploying models on the single-model serving platform].
endif::[]
ifndef::upstream[]
* You have completed the prerequisites in link:{rhoaidocshome}{default-format-url}/deploying_models/deploying_models_on_the_single_model_serving_platform#deploying-models-on-the-single-model-serving-platform_rhoai-user[Deploying models on the single-model serving platform].
endif::[]
ifdef::upstream[]
* The model registry component is enabled in your {productname-short} deployment. For more information, see link:{odhdocshome}/working-with-model-registries/#enabling-the-model-registry-component_model-registry[Enabling the model registry component].
endif::[]
ifdef::self-managed[]
* The model registry component is enabled in your {productname-short} deployment. For more information, see link:{rhoaidocshome}{default-format-url}/enabling_the_model_registry_component[Enabling the model registry component].
endif::[]


.Procedure
. From the {productname-short} dashboard, click *AI hub* -> *Catalog*.
. In the drop-down list, select from the available catalog sources that have been configured by your administrator. The *Default Catalog* is displayed by default. 
+
NOTE: OpenShift cluster administrators can configure additional model catalog sources. For more details, see the Kubeflow Model Registry community documentation on link:https://github.com/kubeflow/model-registry/tree/main/manifests/kustomize/options/catalog#configuring-catalog-sources[configuring catalog sources]. 

. Use the search bar to find a model in the catalog. You can enter text to search by model name, description, or provider.
. Click the name of a model to view the model details page.
. Click *Deploy model* to display the *Deploy model* dialog.
. From the *Project* drop-down list, select a project in which to deploy your model.
+
[NOTE]
====
Models using OCI storage can only be deployed on the single-model serving platform. Projects using the multi-model serving platform do not appear in the project list.
====
. In the *Model deployment* section:
.. Optional: In the *Model deployment name* field, enter a unique name for your model deployment. This field is autofilled with a value that contains the model name by default. 
+
This is the name of the inference service created when the model is deployed.
.. Optional: Click *Edit resource name*, and then enter a specific resource name for the model deployment in the *Resource name* field. By default, the resource name matches the name of the model deployment.
+
[IMPORTANT]
====
Resource names are what your resources are labeled as in OpenShift. Your resource name cannot exceed 253 characters, must consist of lowercase alphanumeric characters or '-', and must start and end with an alphanumeric character. Resource names are not editable after creation.

The resource name must not match the name of any other model deployment resource in your {openshift-platform} cluster.
====
.. From the *Serving runtime* list, select a model-serving runtime that is installed and enabled in your {productname-short} deployment.
If project-scoped runtimes exist, the *Serving runtime* list includes subheadings to distinguish between global runtimes and project-scoped runtimes.
.. From the *Model framework* list, select a framework for your model.
+
NOTE: The *Model framework* list shows only the frameworks that are supported by the model-serving runtime that you specified when you deployed your model.
+
ifndef::upstream[]
. From the **Deployment mode** list, select *KServe RawDeployment* or *Knative Serverless*. For more information about deployment modes, see link:{rhoaidocshome}{default-format-url}/deploying_models/deploying_models_on_the_single_model_serving_platform#about-kserve-deployment-modes_rhoai-user[About KServe deployment modes].
endif::[]
ifdef::upstream[]
. From the **Deployment mode** list, select *KServe RawDeployment* or *Knative Serverless*. For more information about deployment modes, see link:{odhdocshome}/deploying-models/#about-kserve-deployment-modes_odh-user[About KServe deployment modes].
endif::[]
.. In the *Number of model server replicas to deploy* field, specify a value.
.. From the *Model server size* list, select a value.
.. If you have created a hardware profile, select a hardware profile from the *Hardware profile* list.
If project-scoped hardware profiles exist, the *Hardware profile* list includes subheadings to distinguish between global hardware profiles and project-scoped hardware profiles.
+
[IMPORTANT]
====
By default, hardware profiles are hidden from appearing in the dashboard navigation menu and user interface. In addition, user interface components associated with the deprecated accelerator profiles functionality are still displayed. To show the *Settings -> Environment setup -> Hardware profiles* option in the dashboard navigation menu and the user interface components associated with hardware profiles, set the `disableHardwareProfiles` value to `false` in the `OdhDashboardConfig` custom resource (CR) in {openshift-platform}. 
ifdef::upstream[]
For more information about setting dashboard configuration options, see link:{odhdocshome}/managing-resources/#customizing-the-dashboard[Customizing the dashboard].
endif::[]
ifndef::upstream[]
For more information about setting dashboard configuration options, see link:{rhoaidocshome}{default-format-url}/managing_resources/customizing-the-dashboard[Customizing the dashboard].
endif::[] 
====
..  In the *Model route* section, select the *Make deployed models available through an external route* checkbox to make your deployed models available to external clients.
.. In the *Token authentication* section, select the *Require token authentication* checkbox to require token authentication for your model server. To finish configuring token authentication, perform the following actions:
... In the *Service account name* field, enter a service account name for which the token will be generated. The generated token is created and displayed in the *Token secret* field when the model server is configured.
... To add an additional service account, click *Add a service account* and enter another service account name.
. In the *Source model location* section, select *Current URI* to deploy the selected model from the catalog.
. Optional: Customize the runtime parameters in the *Configuration parameters* section:
.. Modify the values in *Additional serving runtime arguments* to define how the deployed model behaves.
.. Modify the values in *Additional environment variables* to define variables in the model's environment.
. Click *Deploy*.

.Verification
* The model deployment is displayed on the *AI hub* -> *Deployments* page.
* The model deployment is displayed in the *Latest deployments* section of the model details page.
* The model deployment is displayed on the *Deployments* tab for the model version.

[role="_additional-resources"]
.Additional resources
ifdef::upstream[]
* For more information about deployment options on the single-model serving platform, see link:{odhdocshome}/deploying-models/#deploying-models-on-the-single-model-serving-platform_odh-user[Deploying models on the single-model serving platform].
endif::[]
ifndef::upstream[]
* For more information about deployment options on the single-model serving platform, see link:{rhoaidocshome}{default-format-url}/deploying_models/deploying_models_on_the_single_model_serving_platform#deploying-models-on-the-single-model-serving-platform_rhoai-user[Deploying models on the single-model serving platform].
endif::[]
