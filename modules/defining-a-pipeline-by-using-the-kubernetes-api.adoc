:_module-type: PROCEDURE

[id="defining-a-pipeline-by-using-the-kubernetes-api_{context}"]
= Defining a pipeline by using the Kubernetes API

[role='_abstract']

You can define data science pipelines and pipeline versions by using the Kubernetes API, which stores them as custom resources in the cluster instead of the internal database. This approach makes it easier to use OpenShift GitOps (Argo CD) or similar tools to manage pipelines and pipeline versions, while still allowing you to manage them through the {productname-short} dashboard, API, and the Kubeflow Pipelines (KFP) Software Development Kit (SDK).

[NOTE]
====
If your pipeline server is already configured to use Kubernetes API storage, you can still use the {productname-short} dashboard and REST API to view pipeline details, run pipelines, and create schedules. In this mode, the Kubernetes API acts as the storage backend, so your existing tools continue to work as expected.
====

.Prerequisites
* You have {productname-short} administrator privileges or you are the project owner.
* You have a data science project with a running pipeline server.
* You have installed the {openshift-cli} as described in the appropriate documentation for your cluster:
ifdef::upstream,self-managed[]
** link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Container Platform  
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-productname}
endif::[]
ifdef::cloud-service[]
** link:https://docs.redhat.com/en/documentation/openshift_dedicated/{osd-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Dedicated  
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws_classic_architecture/{rosa-classic-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-classic-productname}
endif::[]
* If you plan to create a `PipelineVersion` custom resource, you have compiled your Python pipeline into YAML format by using the KFP SDK. For more information, see _Generating the pipeline YAML with the Kubeflow Pipelines SDK_.

.Procedure

. In a terminal window, log in to your {openshift-platform} cluster by using the {openshift-cli}:
+
[source,subs="+quotes"]
----
$ oc login -u __<user_name>__
----
+
When prompted, enter the {openshift-platform} server URL, connection type, and your password.

. To configure the pipeline server to use Kubernetes API storage instead of the default `database` option, set the `spec.apiServer.pipelineStore` field to `kubernetes` in your project's `DataSciencePipelinesApplication` (DSPA) custom resource.
+
In the following command, replace __<dspa_name>__ with the name of your DSPA custom resource, and replace __<namespace>__ with the name of your project:
+
[source,subs="+quotes"]
----
$ oc patch dspa __<dspa_name>__ -n __<namespace>__ \
  --type=merge \
  -p '{"spec": {"apiServer": {"pipelineStore": "kubernetes"}}}'
----
+
[WARNING]
====
When you switch the pipeline server from database storage to Kubernetes API storage, existing pipelines that were stored in the internal database are no longer visible in the {productname-short} dashboard or REST API. To view or manage those pipelines again, change the `spec.apiServer.pipelineStore` field back to `database`.
====

. Define a `Pipeline` custom resource in a YAML file with the following contents:
+
.Example pipeline definition
[source,yaml]
----
apiVersion: pipelines.kubeflow.org/v2beta1
kind: Pipeline
metadata:
  name: <name>
  namespace: <namespace>
spec:
  displayName: <displayName>
----
+
* `name`: The immutable Kubernetes resource name of your pipeline.
* `namespace`: The name of your project.
* `displayName`: The user-friendly display name of your pipeline, which is shown in the dashboard and REST API.

. Apply the pipeline definition to create the `Pipeline` custom resource in your cluster.
+
In the following command, replace __<pipeline_yaml_file>__ with the name of your YAML file:
+
.Example command
[source,subs="+quotes"]
----
$ oc apply -f __<pipeline_yaml_file>__.yaml
----

. Define a `PipelineVersion` custom resource in a YAML file with the following contents:
+
.Example pipeline version definition
[source,yaml]
----
apiVersion: pipelines.kubeflow.org/v2beta1
kind: PipelineVersion
metadata:
  name: <name>
  namespace: <namespace>
spec:
  pipelineName: <pipelineName>
  displayName: <displayName>
  description: This is the first version of the pipeline.
  pipelineSpec: 
        # ... YAML generated by compiling Python pipeline with KFP SDK ...
----
+
* `name`: The name of your pipeline version.
* `namespace`: The name of your project.
* `pipelineName`: The immutable Kubernetes resource name of your pipeline. This value must match the `metadata.name` value in the `Pipeline` custom resource.
* `displayName`: The user-friendly display name of your pipeline version, which is shown in the dashboard and REST API.
* `pipelineSpec`: The YAML content that you generated by using the Kubeflow Pipelines (KFP) SDK.

. Apply the pipeline version definition to create the `PipelineVersion` custom resource in your cluster.
+
In the following command, replace __<pipeline_version_yaml_file>__ with the name of your YAML file:
+
.Example command
[source,subs="+quotes"]
----
$ oc apply -f __<pipeline_version_yaml_file>__.yaml
----
+
After creating the pipeline version, the system automatically applies the following labels to the pipeline version for easier filtering: 
+
.Example automatic labels
[source,yaml]
----
pipelines.kubeflow.org/pipeline-id: <metadata.uid of the pipeline>
pipelines.kubeflow.org/pipeline: <pipeline name>
----

.Verification
. Check that the `Pipeline` custom resource was successfully created:
+
[source,subs="+quotes"]
----
$ oc get pipeline __<pipeline_name>__ -n __<namespace>__
----

. Check that the `PipelineVersion` custom resource was successfully created:
+
[source,subs="+quotes"]
----
$ oc get pipelineversion __<pipeline_version_name>__ -n __<namespace>__
----